# -*- coding: utf-8 -*-
"""online_sales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16OPVr7dwVj38wF_wv4tsRJd_PDhCjOOz
"""

# Install required libraries (run this cell if using Google Colab)
!pip install pandas numpy matplotlib seaborn scikit-learn mlxtend plotly -q

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# For clustering
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA

# For association rules
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# For visualization
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
plt.style.use('seaborn-v0_8-darkgrid')

print("All libraries imported successfully!")

import os
from dotenv import load_dotenv

# Load variables from .env file
load_dotenv()

# Get the values from the environment
kaggle_username = os.getenv('KAGGLE_USERNAME')
kaggle_key = os.getenv('KAGGLE_KEY')

# Set them for the Kaggle API to use
os.environ['KAGGLE_USERNAME'] = kaggle_username
os.environ['KAGGLE_KEY'] = kaggle_key

# Install Kaggle (if not done yet) (remove # if mot done and download it)
#!pip install -q kaggle

# Download the Online Sales dataset
!kaggle datasets download -d yusufdelikkaya/online-sales-dataset --force

import zipfile

with zipfile.ZipFile("online-sales-dataset.zip", 'r') as zip_ref:
    zip_ref.extractall(".")

# Load dataset into dataframe
df = pd.read_csv("online_sales_dataset.csv")

# Preview the dataset
print("DATASET OVERVIEW")
print(f"Dataset shape: {df.shape}")
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

# Looking at first 5 rows
print("FIRST 5 ROWS")
df.head()

# Understand the columns
print("Column Descriptions:")
print("- InvoiceNo: Invoice number (unique per transaction)")
print("- StockCode: Product/item code")
print("- Description: Product/item name")
print("- Quantity: Number of units purchased")
print("- InvoiceDate: Date and time of the transaction")
print("- UnitPrice: Price per unit (before discount)")
print("- CustomerID: Unique identifier for each customer")
print("- Country: Customer's country")
print("- Discount: Discount applied to the product")
print("- PaymentMethod: How the payment was made (e.g., Credit Card, PayPal)")
print("- ShippingCost: Cost of shipping for the transaction")
print("- Category: Product category (e.g., Electronics, Clothing)")
print("- SalesChannel: Where the sale occurred (e.g., Online, In-Store)")
print("- ReturnStatus: Indicates if the product was returned")
print("- ShipmentProvider: Delivery/shipping company used")
print("- WarehouseLocation: Where the item was shipped from")
print("- OrderPriority: Priority level of the order (e.g., High, Medium, Low)")

# Convert InvoiceDate to datetime if not already
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Basic statistics
print("\n Basic Dataset Statistics:")
print(f"- Total transactions: {len(df)}")
print(f"- Unique customers: {df['CustomerID'].nunique()}")
print(f"- Unique products: {df['Description'].nunique()}")
print(f"- Date range: {df['InvoiceDate'].min().date()} to {df['InvoiceDate'].max().date()}")
print(f"- Total countries: {df['Country'].nunique()}")
print(f"- Total categories: {df['Category'].nunique()}")
print(f"- Total sales channels: {df['SalesChannel'].nunique()}")

df.info()

df.describe()

"""Data Cleaning"""

# Check missing values
print("Missing values per column:")
print(df.isnull().sum())

# Print shape before
print("Shape before removing NaNs:", df.shape)

# Drop rows with NaNs in specific columns
rows_before = df.shape[0]
df = df.dropna(subset=['CustomerID', 'ShippingCost', 'WarehouseLocation'])
rows_after = df.shape[0]

# Print shape after and how many rows were removed
print("Shape after removing NaNs:", df.shape)
print("Total rows removed:", rows_before - rows_after)

# Print shape before
print("Shape before removing returns:", df.shape)

# Remove returned transactions and calculate how many were dropped
print("Total rows removed:", df.shape[0] - df[df['ReturnStatus'].str.lower() != 'returned'].shape[0])

# Apply the filter
df = df[df['ReturnStatus'].str.lower() != 'returned']

# Print shape after
print("Shape after removing returns:", df.shape)

# Print shape before filtering
print("Shape before removing non-positive Quantity/UnitPrice:", df.shape)

# Print how many rows will be removed
print("Total rows removed:",
      df.shape[0] - df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)].shape[0])

# Apply the filter
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]

# Print shape after
print("Shape after removing non-positive Quantity/UnitPrice:", df.shape)

# Create the TotalPrice column
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

# Confirm it was added
print("New column 'TotalPrice' added.")
print(df[['Quantity', 'UnitPrice', 'TotalPrice']].head())

df.drop(columns=[
    'ReturnStatus', 'ShipmentProvider', 'WarehouseLocation',
    'OrderPriority', 'PaymentMethod', 'ShippingCost',
    'SalesChannel', 'Discount', 'StockCode'
], inplace=True)

print("Dropped unnecessary columns.")
print("Remaining columns:", df.columns.tolist())

# Save the cleaned and feature-engineered DataFrame to a CSV file
df.to_csv('cleaned_customer_data.csv', index=False)
print("File saved as cleaned_customer_data.csv")

from google.colab import files
files.download('cleaned_customer_data.csv')

"""RMF Analysis"""

from datetime import timedelta

# Define the reference date (latest date in your dataset + 1 day)
reference_date = df['InvoiceDate'].max() + timedelta(days=1)
print(f"Reference date for recency calculation: {reference_date}")

# Create RFM table
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency
    'InvoiceNo': 'nunique',  # Frequency (number of purchases)
    'TotalPrice': 'sum'  # Monetary value
})

# Rename columns
rfm.columns = ['Recency', 'Frequency', 'Monetary']

# Reset index
rfm = rfm.reset_index()

# Display summary and sample data
print("\nRFM Table Summary:")
print(rfm.describe())

print("\nSample RFM data:")
print(rfm.head(10))

# Add additional customer features

# 1. Average Order Value
rfm['AvgOrderValue'] = rfm['Monetary'] / rfm['Frequency']

# 2. Number of unique products purchased per customer
products_per_customer = df.groupby('CustomerID')['Description'].nunique().reset_index()
products_per_customer.columns = ['CustomerID', 'UniqueProducts']
rfm = rfm.merge(products_per_customer, on='CustomerID')

# Redefine total transactions per customer using unique InvoiceNo (actual purchases)
transactions_per_customer = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index(name='TotalTransactions')

# Drop old version if it exists
rfm = rfm.drop(columns=['TotalTransactions', 'ProductDiversity'], errors='ignore')

# Merge correct transaction counts
rfm = rfm.merge(transactions_per_customer, on='CustomerID')

# Recalculate diversity score
rfm['ProductDiversity'] = rfm['UniqueProducts'] / rfm['TotalTransactions']

# Output result
print("\nEnhanced customer features:")
print(rfm.columns.tolist())

print("\nSample enhanced data:")
print(rfm.head())

print("Customers with ProductDiversity = 0:", (rfm['ProductDiversity'] == 0).sum())
print("Lowest diversity samples:")
print(rfm[['CustomerID', 'UniqueProducts', 'TotalTransactions', 'ProductDiversity']]
      .sort_values('ProductDiversity')
      .head(10))

"""Visualize RFM Distributions"""

# Create visualizations for RFM and behavioral metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Distribution of Customer Features', fontsize=16)

# Flatten axes for easier indexing
axes = axes.flatten()

# Recency
axes[0].hist(rfm['Recency'], bins=50, color='skyblue', edgecolor='black')
axes[0].set_title('Recency (Days since last purchase)')
axes[0].set_xlabel('Days')
axes[0].set_ylabel('Number of Customers')

# Frequency (0 to 10)
axes[1].hist(rfm['Frequency'], bins=50, color='lightgreen', edgecolor='black')
axes[1].set_title('Frequency (Number of purchases)')
axes[1].set_xlabel('Number of Purchases')
axes[1].set_xlim(0, 10)

# Monetary
axes[2].hist(rfm['Monetary'], bins=50, color='salmon', edgecolor='black')
axes[2].set_title('Monetary (Total spending)')
axes[2].set_xlabel('Total Spending ($)')
axes[2].set_xlim(0, 10000)

# Average Order Value (0 to 5000)
axes[3].hist(rfm['AvgOrderValue'], bins=50, color='gold', edgecolor='black')
axes[3].set_title('Average Order Value')
axes[3].set_xlabel('Average Order Value ($)')
axes[3].set_xlim(0, 5000)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""Standardize the RFM + Behavioral Features"""

# Step 1: Select features for clustering
clustering_features = ['Recency', 'Frequency', 'Monetary', 'AvgOrderValue', 'UniqueProducts']
X = rfm[clustering_features]

# Step 2: Remove outliers using IQR method
Q1 = X.quantile(0.25)
Q3 = X.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("Removing outliers based on IQR...")
mask = ~((X < lower_bound) | (X > upper_bound)).any(axis=1)

X_clean = X[mask]
rfm_clean = rfm[mask]  # Use this in future analysis

print(f"Original data: {len(X)} customers")
print(f"Remaining after outlier removal: {len(X_clean)}")
print(f"Outliers removed: {len(X) - len(X_clean)} ({(len(X) - len(X_clean)) / len(X) * 100:.2f}%)")

# Step 3: Feature scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clean)

print("\nFeatures have been standardized using StandardScaler.")

"""Determine Optimal Number of Clusters (Elbow Method)"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np  # make sure this is imported

# Elbow and Silhouette Method to find optimal k
wcss = []
silhouette_scores = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Plot elbow curve and silhouette scores
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Elbow curve
ax1.plot(K, wcss, 'bo-')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)')
ax1.set_title('Elbow Method For Optimal k')
ax1.grid(True)

# Silhouette scores
ax2.plot(K, silhouette_scores, 'ro-')
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Score For Different k')
ax2.grid(True)

plt.tight_layout()
plt.show()

# Find optimal k based on silhouette score
optimal_k = K[np.argmax(silhouette_scores)]
print(f"\nOptimal number of clusters based on Silhouette Score: {optimal_k}")
print(f"Silhouette Score for k={optimal_k}: {max(silhouette_scores):.3f}")

"""Apply K-Means with Chosen K and Visualizations Clusters with PCA"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd

# Define the features used in clustering
clustering_features = ['Recency', 'Frequency', 'Monetary', 'AvgOrderValue']

# Scale only the selected clustering features
scaler_cluster = StandardScaler()
scaled_clustering_df = scaler_cluster.fit_transform(rfm[clustering_features])

# Run KMeans with optimal number of clusters
k_optimal = 2
kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)
rfm['Cluster'] = kmeans.fit_predict(scaled_clustering_df)

# Print cluster sizes
print("\nCluster Sizes:")
cluster_counts = rfm['Cluster'].value_counts().sort_index()
for cluster, count in cluster_counts.items():
    print(f"Cluster {cluster}: {count} customers ({count / len(rfm) * 100:.1f}%)")

# Inverse transform the cluster centers to original scale
cluster_centers = scaler_cluster.inverse_transform(kmeans.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers, columns=clustering_features)
cluster_centers_df['Cluster'] = range(k_optimal)

print("\nCluster Centers (Original Scale):")
print(cluster_centers_df.round(2))

# Perform PCA to reduce to 2 dimensions for visualization
pca = PCA(n_components=2)
pca_components = pca.fit_transform(scaled_clustering_df)

rfm['PCA1'] = pca_components[:, 0]
rfm['PCA2'] = pca_components[:, 1]

# Plot PCA results
plt.figure(figsize=(8, 6))
for cluster in sorted(rfm['Cluster'].unique()):
    subset = rfm[rfm['Cluster'] == cluster]
    plt.scatter(subset['PCA1'], subset['PCA2'], label=f'Cluster {cluster}', alpha=0.6)

plt.xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('Customer Segments (PCA Projection)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Total explained variance
print(f"\nTotal variance explained by 2 PCA components: {sum(pca.explained_variance_ratio_):.1%}")

"""### Analyze Cluster Characteristics"""

# Ensure Cluster column exists in rfm_clean
rfm_clean['Cluster'] = rfm['Cluster']  # Add this line

# Calculate cluster profiles
cluster_profiles = rfm_clean.groupby('Cluster')[clustering_features].mean().round(2)
print("Average Values by Cluster:\n", cluster_profiles)

# Normalize for radar chart
normalized_profiles = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())

# Drop columns with no variation (e.g., Frequency = 1 for all)
normalized_profiles = normalized_profiles.loc[:, normalized_profiles.std() > 0]
categories = normalized_profiles.columns.tolist()

# List all unique clusters
print("Clusters present:", rfm_clean['Cluster'].unique())

# Inspect customers in Cluster 2 (if any)
cluster_2_data = rfm_clean[rfm_clean['Cluster'] == 2]
print("Number of customers in Cluster 2:", cluster_2_data.shape[0])

# Setup for radar chart
from math import pi
import matplotlib.pyplot as plt

num_vars = len(categories)
angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]
angles += angles[:1]  # complete the loop

fig = plt.figure(figsize=(10, 8))
ax = plt.subplot(111, polar=True)

# Plot each cluster
colors = ['blue', 'orange', 'green', 'red', 'purple']
for idx, row in normalized_profiles.iterrows():
    values = row.tolist()
    values += values[:1]
    ax.plot(angles, values, label=f'Cluster {idx} ({rfm_clean[rfm_clean["Cluster"] == idx].shape[0]} customers)',
            linewidth=2, color=colors[idx % len(colors)])
    ax.fill(angles, values, alpha=0.25, color=colors[idx % len(colors)])

# Add labels
plt.xticks(angles[:-1], categories)
plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], color="grey", size=8)
plt.title('Cluster Profiles (Normalized)', size=16, y=1.08)
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

plt.tight_layout()
plt.show()

# Cluster Obsertaions for each cluster
print("CLUSTER OBSERVATIONS:")

# Cluster 0
print("\nCluster 0 (5938 customers)")
print("1. Recency: Purchased more recently than Cluster 1.")
print("2. Frequency: Made only one purchase.")
print("3. Monetary: Spent a high total amount (~$2717).")
print("4. AvgOrderValue: Very high average order value (~$2717).")

# Cluster 1
print("\nCluster 1 (19387 customers)")
print("1. Recency: Haven’t purchased in a long time.")
print("2. Frequency: Also made only one purchase.")
print("3. Monetary: Spent significantly less (~$727).")
print("4. AvgOrderValue: Lower average order value (~$727).")

"""Create Customer Personas"""

# Required features for clustering
clustering_features = ['Recency', 'Frequency', 'Monetary', 'AvgOrderValue', 'UniqueProducts']
rfm_features = rfm_clean[clustering_features].copy()

# Rebuild rfm_full (excluding PCA for now)
rfm_full = rfm_features.copy()
rfm_full['CustomerID'] = rfm_clean['CustomerID']
rfm_full['Cluster'] = rfm_clean['Cluster']

# Define customer personas
def define_persona(cluster_metrics):
    recency = cluster_metrics['Recency']
    frequency = cluster_metrics['Frequency']
    monetary = cluster_metrics['Monetary']
    avg_order = cluster_metrics['AvgOrderValue']
    diversity = cluster_metrics['UniqueProducts']

    if recency < 60 and frequency >= 20 and monetary >= 3000:
        return "Champions", "Highly engaged, frequent buyers with high spending and recent activity"
    elif recency < 100 and frequency >= 10 and monetary >= 1500:
        return "Loyal Customers", "Repeat customers with consistent purchases and solid spend"
    elif recency > 300 and monetary >= 1000:
        return "At Risk", "Previously valuable customers who have not shopped recently"
    elif frequency < 5 and monetary < 500:
        return "New or Low-Value Customers", "Recently acquired or inactive customers with minimal activity"
    else:
        return "Potential Loyalists", "Moderate activity and spending, could become loyal with targeted incentives"

# Group by cluster (exclude CustomerID from mean)
cluster_profiles = rfm_full.drop(columns='CustomerID').groupby('Cluster').mean(numeric_only=True)
cluster_sizes = rfm_full['Cluster'].value_counts().sort_index()

# Assign personas
personas = {}
for cluster_id, row in cluster_profiles.iterrows():
    persona_name, description = define_persona(row)
    personas[cluster_id] = {
        'name': persona_name,
        'description': description,
        'size': cluster_sizes[cluster_id],
        'metrics': row.to_dict()
    }

# Print results
print("CUSTOMER PERSONAS BY CLUSTER\n")
for cluster_id, persona in personas.items():
    print(f"\nCluster {cluster_id}: {persona['name']}")
    print(f"Description: {persona['description']}")
    print(f"Size: {persona['size']} customers")
    print("Key Metrics:")
    for metric, value in persona['metrics'].items():
        print(f"  - {metric}: {value:.2f}")

"""Hierarchical Clustering"""

from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import adjusted_rand_score
import numpy as np
import matplotlib.pyplot as plt

# ----- HIERARCHICAL DENDROGRAM (Sample for visualization) -----
# Sample a subset for the dendrogram (avoid clutter)
sample_indices = np.random.choice(len(X_scaled), size=min(100, len(X_scaled)), replace=False)
X_sample = X_scaled[sample_indices]

# Perform hierarchical clustering (Ward linkage)
linkage_matrix = linkage(X_sample, method='ward')

# Plot the dendrogram
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.title('Hierarchical Clustering Dendrogram (Sample)')
plt.xlabel('Customer Index')
plt.ylabel('Distance')
plt.show()

# ----- AGGLOMERATIVE CLUSTERING -----
# Use the same number of clusters as KMeans
optimal_k = 2  # Based on your KMeans results

agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)
agg_clusters = agg_clustering.fit_predict(X_scaled)

# Save the results
rfm_clean['HC_Cluster'] = agg_clusters

# Optional: Compare with KMeans
from sklearn.metrics import adjusted_rand_score
similarity = adjusted_rand_score(rfm_clean['Cluster'], rfm_clean['HC_Cluster'])
print(f"\nSimilarity between K-Means and Hierarchical Clustering: {similarity:.3f}")
print("(1.0 = identical, 0.0 = random)")

"""Market Basket Analysis with FP-Growth algorithm"""

# Create transaction matrix: rows = CustomerID, columns = product descriptions
basket = df.groupby(['CustomerID', 'Description'])['Quantity'].sum().unstack().fillna(0)

# Convert quantities to binary (1 = product purchased, 0 = not purchased)
basket_sets = basket.applymap(lambda x: 1 if x > 0 else 0)

# Overview
print(f"Transaction matrix shape: {basket_sets.shape}")
print(f"Number of transactions (customers): {basket_sets.shape[0]}")
print(f"Number of unique products: {basket_sets.shape[1]}")

# Show sample matrix
basket_sets.iloc[:5, :5]

"""Apply FP-Growth Algorithm"""

from mlxtend.frequent_patterns import fpgrowth

# Set minimum support threshold (tune based on dataset)
min_support = 0.003  # 0.3% of all transactions

# Generate frequent itemsets
frequent_itemsets = fpgrowth(basket_sets, min_support=min_support, use_colnames=True)

# Add a column for length of each itemset
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))

# Summary: how many itemsets of each length?
print("Frequent Itemsets by Length:")
print(frequent_itemsets['length'].value_counts())

# Preview top itemsets
print("\nTop Frequent Itemsets:")
display(frequent_itemsets.sort_values(by='support', ascending=False).head(10))

"""Generate Association Rules"""

from mlxtend.frequent_patterns import association_rules

# Set minimum confidence threshold (tune based on dataset)
min_confidence = 0.01

# Generate rules from frequent itemsets
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_confidence)

# Show how many rules were generated
print(f"\nGenerated {len(rules)} association rules")
# Add length columns to analyze complexity of rules
rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))
rules['consequent_len'] = rules['consequents'].apply(lambda x: len(x))

"""Display Top Rules by Lift"""

# Sort rules by lift and show top 10
top_rules = rules.sort_values('lift', ascending=False).head(10)

print("\nTop 10 Association Rules by Lift:")
for idx, rule in top_rules.iterrows():
    antecedent = ', '.join(list(rule['antecedents']))
    consequent = ', '.join(list(rule['consequents']))
    print(f"\nRule {idx}:")
    print(f"  If customer buys: {antecedent}")
    print(f"  Then likely to buy: {consequent}")
    print(f"  Support: {rule['support']:.3f}")
    print(f"  Confidence: {rule['confidence']:.3f}")
    print(f"  Lift: {rule['lift']:.3f}")

"""Visualize the Rules"""

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
scatter = plt.scatter(rules['support'], rules['confidence'],
                      c=rules['lift'], s=rules['lift']*20,
                      alpha=0.6, cmap='viridis')
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Association Rules: Support vs Confidence (Lift = Color & Size)')
plt.colorbar(scatter, label='Lift')
plt.grid(True, alpha=0.3)
plt.show()

"""Product Recommendation Engine"""

def get_recommendations(product, rules_df, max_recommendations=5):
    """
    Generate product recommendations based on association rules.

    Args:
        product (str): The input product to base recommendations on.
        rules_df (DataFrame): Association rules DataFrame.
        max_recommendations (int): Max number of recommendations to return.

    Returns:
        DataFrame of recommended products with confidence and lift.
    """
    # Find rules where the product appears in the antecedents
    relevant_rules = rules_df[rules_df['antecedents'].apply(lambda x: product in x)]

    if len(relevant_rules) == 0:
        return "No recommendations found for this product."

    # Sort by lift and return top rules
    relevant_rules = relevant_rules.sort_values('lift', ascending=False).head(max_recommendations)

    recommendations = []
    for idx, rule in relevant_rules.iterrows():
        consequent = ', '.join(list(rule['consequents']))
        recommendations.append({
            'Product': consequent,
            'Confidence': f"{rule['confidence']:.1%}",
            'Lift': f"{rule['lift']:.2f}"
        })

    return pd.DataFrame(recommendations)

"""Choose a Test Product and Show Recommendations"""

# Identify frequently purchased products (you can adjust number)
popular_products = df['Description'].value_counts().head(20).index.tolist()

print("Product Recommendation Engine Demo")

# Pick a product to test (first most popular)
test_product = popular_products[0]
print(f"\nRecommendations for: {test_product}")
display(get_recommendations(test_product, rules))

"""Business Insights and Recommendations"""

# Merge Cluster Info into Cleaned Transaction Data
# This allows us to analyze product purchasing behavior per cluster
df_with_clusters = df.merge(rfm_clean[['CustomerID', 'Cluster']], on='CustomerID')

# Sum quantity of products purchased per product per cluster
cluster_products = df_with_clusters.groupby(['Cluster', 'Description'])['Quantity'].sum().reset_index()

# Sort so top-selling products appear first per cluster
cluster_products = cluster_products.sort_values(['Cluster', 'Quantity'], ascending=[True, False])

print("TOP 5 PRODUCTS BY CUSTOMER SEGMENT")

for cluster in range(optimal_k):
    print(f"\nCluster {cluster} - {personas[cluster]['name']}:")

    top_products = cluster_products[cluster_products['Cluster'] == cluster].head(5)

    for _, row in top_products.iterrows():
        print(f"  - {row['Description']}: {row['Quantity']} units")

print("\nBUSINESS RECOMMENDATIONS")

for cluster_id, persona in personas.items():
    print(f"\n{persona['name']} (Cluster {cluster_id})")

    metrics = persona['metrics']

    if metrics['Recency'] < 50 and metrics['Frequency'] > 15:
        print("RETAIN: These are your best customers")
        print("  - Offer VIP programs and exclusive early access")
        print("  - Send personalized thank you messages")
        print("  - Provide premium customer service")

    elif metrics['Recency'] > 150 and metrics['Monetary'] > 1000:
        print("RE-ENGAGE: High-value customers at risk of churning")
        print("  - Send win-back campaigns with special offers")
        print("  - Conduct surveys to understand why they left")
        print("  - Offer personalized discounts on previous items")

    elif metrics['Frequency'] < 5:
        print("DEVELOP: New or low-engagement customers")
        print("  - Create onboarding email sequences")
        print("  - Offer first-time buyer discounts")
        print("  - Recommend popular products")

    else:
        print("GROW: Customers with growth potential")
        print("  - Cross-sell complementary products")
        print("  - Implement loyalty programs")
        print("  - Send targeted product recommendations")

    avg_value = metrics['Monetary'] / metrics['Frequency'] if metrics['Frequency'] > 0 else 0
    potential_revenue = persona['size'] * avg_value * 0.2  # 20% conversion assumed
    print(f"\n  Potential Revenue Impact: ${potential_revenue:,.0f}")

# Cross-selling opportunities from association rules
print("\n\nCROSS-SELLING OPPORTUNITIES")

# Try to find strong rules (if any)
strong_rules = rules[
    (rules['lift'] > 1.5) &
    (rules['confidence'] > 0.3) &
    (rules['antecedent_len'] == 1)
].sort_values('lift', ascending=False).head(10)

if strong_rules.empty:
    print("No strong cross-selling rules found based on the current data and thresholds.")
    print("Consider using lower support thresholds or a larger dataset.")
else:
    print("\nTop 10 Cross-Selling Opportunities:")
    for idx, (_, rule) in enumerate(strong_rules.iterrows(), 1):
        antecedent = list(rule['antecedents'])[0]
        consequent = list(rule['consequents'])[0]

        print(f"\n{idx}. When customers buy '{antecedent}'")
        print(f"   Recommend '{consequent}'")
        print(f"   Confidence: {rule['confidence']:.1%} | Lift: {rule['lift']:.1f}x")

        # Calculate potential impact
        support_increase = rule['support'] * rule['lift']
        print(f"   Potential sales increase: {support_increase:.2%}")

print("\nSaving analysis results...")

rfm_clean.to_csv('customer_segments.csv', index=False)
print("Customer segments saved to 'customer_segments.csv'")

rules.to_csv('association_rules.csv', index=False)
print("Association rules saved to 'association_rules.csv'")

cluster_profiles.to_csv('cluster_profiles.csv')
print("Cluster profiles saved to 'cluster_profiles.csv'")

print("\nAnalysis complete! Results have been saved for deployment.")

# Save your dataframes to CSVs locally
rfm_clean.to_csv("customer_segments.csv", index=False)
rules.to_csv("association_rules.csv", index=False)
cluster_profiles.to_csv("cluster_profiles.csv", index=False)

print("Files saved locally:")
print("- customer_segments.csv")
print("- association_rules.csv")
print("- cluster_profiles.csv")

from google.colab import files

# Download the files to your laptop
files.download("customer_segments.csv")
files.download("association_rules.csv")
files.download("cluster_profiles.csv")
